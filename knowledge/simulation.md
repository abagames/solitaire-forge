# Simulation Execution and Evaluation Feedback Creation Instructions

## 1. Purpose

This document provides guidelines for evaluating the definition of a single-player card game generated by an LLM (Large Language Model) and structuring the results as feedback to the LLM.

The goal is to streamline the collaborative game design process with the LLM and improve the accuracy of game rule refinement by using consistent evaluation criteria and a clear feedback format.

## 2. Simulation Execution Guide

### 2.1. Execution Method

Each game has a dedicated simulator file (e.g., `src/games/[game-name]/simulator.ts`). This file is executed directly with Node.js. `ts-node` is required for execution.

```bash
# Example: For the cartographers-expedition game
node --loader ts-node/esm src/games/cartographers-expedition/simulator.ts
```

**Setting up a new game's simulator:**

1.  Copy `src/simulator_template.ts` and place it in the new game's directory (e.g., `src/games/[new-game-name]/`) as `simulator.ts`.
2.  Open the copied `simulator.ts` file and edit the following points according to the instruction comments at the beginning of the file:
    - Uncomment the `import` statements and correct the paths to import the game definition file (`definition.ts` or `index.ts`) and the AI file (`ai.ts`).
    - In the `main` function where `runSimulations` is called, replace the placeholders (`YourGameDefinition`, `YourGameBasicAI`) with the imported ones.
    - Change the seed name prefix (`simulationSeedPrefix`) to match the game name.
    - Adjust the AI name display in `printSummary` if necessary.

**Running simulations for existing games:**

Open the target game's `simulator.ts` file (e.g., `src/games/[game-name]/simulator.ts`), ensure the game definition and AI are correctly imported and referenced in the `main` function, then run the `node` command above.

This script simulates the game for the number of times defined by `NUM_TRIALS` in the file, using **both** `RandomAI` and `game-specific BasicAI`, aggregates the results for each, and outputs them to the console.

### 2.2. Player AI

The simulator (`src/simulator.ts`) typically evaluates using the following two types of AI.

- **Random AI (`RandomAI`):**
  - **Behavior:** Always randomly selects one from the available valid moves.
  - **Purpose:** Measures basic rule executability and achievement by luck alone (baseline). It serves as an indicator of the game's minimum difficulty and how prone it is to dead ends.
- **Basic Strategy AI (`game-specific BasicAI`, e.g., `CartographersBasicAI`):**
  - **Behavior:** Selects actions based on a relatively simple strategy logic defined for each game. (e.g., choosing the move closest to the goal, prioritizing specific cards). It usually does not perform complex state retention like "memory."
  - **Purpose:** Evaluates the effectiveness of the basic strategy (a simple playstyle likely intended by the designer) and how much the game mechanics contribute to strategic decision-making.

**Importance of Comparison:** Comparing the results of `RandomAI` and `BasicAI` provides important clues to judge how much advantage the introduced strategic elements offer over random play, and the extent to which skill is a factor in the game.

### 2.3. Number of Trials

- **Default Count:** Defined by the `NUM_TRIALS` constant in each game's `simulator.ts`, defaulting to **1000 trials**. Change this value and run if necessary. Running more trials improves statistical reliability.

### 2.4. Reproducibility

- **Deterministic Seed:** The `simulationSeedRandom` and `simulationSeedBasic` constants in each game's `simulator.ts` define the base seed for each AI's simulation trial group (e.g., `[game-name]-rand-v1`). For each trial and each AI's turn within each trial, a unique seed derived from this base seed and indices (e.g., `baseSeed-trial-i`, `baseSeed-trial-i-turn-j`) is used internally. This allows the entire simulation (game setup and all AI action choices) to be perfectly reproduced using a specific seed value.

## 3. Quantitative Data Collected

The simulation script (`src/games/[game-name]/simulator.ts`) automatically aggregates the following data and outputs it to the console (as `SimulationSummary` type).

- **Total Games (`totalGames`)**
- **Wins (`wins`)**
- **Losses (`losses`)**
- **Win Rate (`winRate`)**: %
- **Loss Statistics by Reason (`lossRateByReason`)**:
  - For each reason string (e.g., `STUCK`, `MAX_MOVES_REACHED`),
    - Occurrence count (`count`)
    - Percentage of total games (`rate`, %)
- **Move Statistics (`movesOverall`, `movesWins`, `movesLosses`)**:
  - For all games, winning games, and losing games respectively,
    - Average moves (`average`)
    - Minimum moves (`min`)
    - Maximum moves (`max`)

## 4. Qualitative Aspects to Evaluate

Evaluate the following aspects through the collected quantitative data and, if possible, several playthroughs by a human.

- **Functionality of Core Mechanics:**
  - Do the mechanics intended as the core of the game actually work? (e.g., If AI action usage frequency shows few actions related to a specific mechanic, it might not be functioning)
  - Can the player understand and utilize those mechanics?
  - Is playing with those mechanics fun, or monotonous?
- **Game Progression and Pacing:**
  - Does the game progress smoothly? Are there phases prone to stagnation? (e.g., If `STUCK` is frequent among loss reasons, it's prone to dead ends)
  - Does the game's nature change in the early, middle, and late stages?
  - Is the duration of one play appropriate? (Judge by average moves/turns)
- **Strategic Diversity:**
  - Do multiple effective strategies or approaches to victory seem to exist?
  - Are specific cards or actions overwhelmingly strong, making other choices meaningless? (Check for bias in AI action usage frequency)
- **Player Choices:**
  - Does the player always have multiple meaningful actions to choose from during the game?
  - Are there situations where choices are difficult? Or are the actions to take often obvious?
- **Difficulty and Sense of Accomplishment:**
  - Is the difficulty appropriate?
  - Is there a sense of accomplishment when winning? Is there a sense of 納得感 (納得感 - natoku-kan: feeling of acceptance/understanding, often for a loss) when losing?
  - If BasicAI's win rate is extremely high (especially close to 100%), it's a significant sign not just that the AI is good, but that the game might be too easy, or a monotonous winning pattern exists. Attention should be paid not only to the win rate but also to how it wins.
- **Balance of Luck and Skill:**
  - Is the game outcome primarily determined by luck, or is it influenced by player judgment?
  - Are there unacceptable levels of luck (e.g., high probability of getting stuck on the first move)?
- **Consistency with Theme:** (If applicable)
  - Does the gameplay (rules, actions, goals) effectively express the set theme/concept?
- **Fun / Frustration:**
  - What are the most fun moments/elements when playing?
  - What are the most boring or frustrating moments/elements? (Infer from loss reasons, low scores, etc.)
- **Rule Issues:**
  - During the simulation, were there any infinite loops, state contradictions, unexpected behaviors, or ambiguities in rule interpretation?
  - Are there any exploitable combinations of rules?
  - Do rules break down in edge cases (special situations)?
